O Q-learning é um algoritmo de aprendizado Por reforço que busca encontrar a melhor ação a ser tomada, dado o estado atual. É considerado off-policy porque a função q-learning aprende com ações que estão fora da política atual, como executar ações aleatórias. Mais especificamente, o q-learning busca aprender uma política que maximize a recompensa total.O ‘q’ no q-learning significa qualidade. A qualidade, neste caso, representa a utilidade de uma determinada ação para obter alguma recompensa futura.Esse é um dos algoritmos mais usados em Aprendizado Por Reforço, o qual vamos estudar agora. É amplamente empregado em robótica, automação de games e robôs investidores na bolsa de valores.Q-learning é um algoritmo de aprendizado baseado em valor. Os algoritmos baseados em valor atualizam a função de valor com base em uma equação (particularmente a equação de Bellman). Enquanto o outro tipo, baseado em políticas, estima a função de valor com uma política gananciosa obtida a partir do último aprimoramento da política.Funções de valor são funções de par de ação de estado que estimam quão boa será uma ação específica em um determinado estado ou qual o retorno esperado para essa ação.O Q-Learning é um algoritmo off-policy (pode atualizar as funções de valor estimado usando ações hipotéticas, aquelas que ainda não foram tentadas) para o aprendizado da diferença temporal (método para estimar as funções de valor). Pode-se provar que, com treinamento suficiente, o Q-learning converge com a probabilidade 1 para uma aproximação da função de valor da ação para uma política de destino arbitrária. O Q-Learning aprende a política ideal, mesmo quando as ações são selecionadas de acordo com uma política mais exploratória ou até aleatória. O Q-learning pode ser implementado da seguinte maneira:Onde:O algoritmo pode ser interpretado como:Aqui o algoritmo:A próxima etapa para o agente é interagir com o ambiente e fazer atualizações nos pares de estado e ação em nossa tabela q: Q [estado, ação].Um agente interage com o ambiente com 1 de 2 maneiras possíveis. A primeira é usar a tabela q como referência e visualizar todas as ações possíveis para um determinado estado. O agente seleciona a ação com base no valor máximo dessas ações. Isso é conhecido como Exploit, pois usamos as informações que temos disponíveis para tomar uma decisão.A segunda maneira é agir aleatoriamente. Isso é chamado de Explore. Em vez de selecionar ações com base na recompensa máxima futura, selecionamos uma ação aleatoriamente. A ação aleatória é importante porque permite ao agente explorar e descobrir novos estados que, de outra forma, não podem ser selecionados durante o processo de Exploit. Você pode equilibrar a Exploit / Explore usando o parâmetro épsilon (ε) e definindo o valor de quantas vezes deseja fazer Exploit versus Explore. O Q-Learning é um algoritmo de aprendizado por reforço (reinforcement learning) proposto por Christopher Watkins em 1989. Esse algoritmo aprende a tomar decisões interagindo diretamente com o ambiente, sem a necessidade de conhecimento prévio ou de supervisão humana. Por esse motivo, o Q-Learning é amplamente aplicado na resolução de problemas complexos e dinâmicos.O Q-learning é um algoritmo off-policy, o que significa que o agente aprende uma política ótima enquanto explora o ambiente de maneira aleatória ou seguindo uma política diferente da política atual. Isso permite que o agente aprenda uma política ótima mesmo durante a exploração inicial. O Q-learning utiliza uma estratégia epsilon-greedy para equilibrar a exploração (realizar ações aleatórias para descobrir o ambiente) e a explotação (selecionar ações com base nos Q-valores aprendidos). Isso permite que o agente descubra novas ações e, ao mesmo tempo, maximize suas recompensas de acordo com o conhecimento atual. O algoritmo Q-learning atualiza iterativamente os Q-valores com base nas recompensas recebidas e nas transições de estado. Ele utiliza a equação de atualização do Q-valor, que combina a recompensa imediata, o Q-valor do próximo estado e um fator de desconto para ponderar as recompensas futuras